Python 2.7.10
I have used BeautifulSoup4 third party library.
Citation:
- Saw https://www.youtube.com/watch?v=XjNm9bazxn8&t=336s to learn how to start off making a crawler


TASK 1:
to run the program, run the following command in command prompt:
python crawler-task1.py

My seed page is a constant defined in the code itself. I am not accepting any parameters in the command promt.
Also my crawled-test1.txt is opened in append mode. So make sure you delete the file if you choose to run the program again.
Orelse it will append the 1000 links to the same file giving you 2000 entries.

If you wish to change the seed link then you will have to open the source code and make changes to the constant: seed_page



Task 2a:
to run the program, run the following command in command prompt:
python focused-search-bfs.py

My seed page and keywords are a constant defined in the code itself. I am not accepting any parameters in the command promt.
Also my crawled-task2-bfs.txt is opened in append mode. So make sure you delete the file if you choose to run the program again.
Orelse it will append the 1000 links to the same file giving you 2000 entries.

If you wish to change the seed link or the search keywords then you will have to open the source code and make changes to the constant: seed_page, keyword1, keyword2



Task 2b:
to run the program, run the following command in command prompt:
python focused-search-dfs.py

My seed page and keywords are a constant defined in the code itself. I am not accepting any parameters in the command promt.
Also my crawled-task2-dfs.txt is opened in append mode. So make sure you delete the file if you choose to run the program again.
Orelse it will append the 1000 links to the same file giving you 2000 entries.

If you wish to change the seed link or the search keywords then you will have to open the source code and make changes to the constant: seed_page, keyword1, keyword2



Task 3:
to run the program, run the following command in command prompt:
python crawler-task3.py

My seed page and keywords are a constant defined in the code itself. I am not accepting any parameters in the command promt.
Also my crawled-task3.txt is opened in append mode. So make sure you delete the file if you choose to run the program again.
Orelse it will append the 1000 links to the same file giving you 2000 entries.

If you wish to change the seed link or the search keywords then you will have to open the source code and make changes to the constant: seed_page.
The explanation and psudo code for task 3 merging process is present in task3.txt.





